{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b861f30-ea7a-45f3-b930-a2c682d930c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# THE OFFICE RAG with Langchain and Llama\n",
    "<hr style=\"border:2px solid black\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bd6c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Black formatting (optional, but recommended for consistent style)\n",
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bcc22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain import hub\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "import numpy as np\n",
    "from langchain.chains import RetrievalQA\n",
    "import textwrap\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e573b56e-82e5-4199-b0e5-39f11dc822f9",
   "metadata": {},
   "source": [
    "#### load credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b28865fe-2a9c-4aee-b877-905950d6b0d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0912fa18-71fc-48b3-9ffe-0ef95a928432",
   "metadata": {},
   "source": [
    "#### define llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33dfb5d4-1c39-4b19-8162-29e78bacd5ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"llama3-8b-8192\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d9acc4-cf08-4399-8c2a-3b1f561a288b",
   "metadata": {},
   "source": [
    "#### define promt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebbd8e0d-8e1a-4443-885e-904d9c03bbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    You are any character from the TV series THE OFFICE. \n",
    "    You will respond to any questions and comments in the style of a random character.\n",
    "    You will not break such character until instructed to do so. \n",
    "    You will not say anything about the show or the characters. \n",
    "    You will only respond as the character. \n",
    "    You may not make reference to people and events in the show.\n",
    "    You will not say anything about events in the show your character knows nothing about or not involved with.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b21913-08f6-40cf-a38c-7e1ae1de0c87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"information\"],\n",
    "    template=query\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfcca91-4eb7-4e91-a243-a707f8df9c66",
   "metadata": {},
   "source": [
    "#### define Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a38301-632a-4211-a314-3faa63ea73f9",
   "metadata": {},
   "source": [
    "**What is a Chain?**\n",
    "\n",
    "> - allows to link the output of one LLM call as the input of another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3af1185b-5103-4017-9c18-2a21e083e8c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = prompt_template | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3746365-ec7d-49e1-b130-5dcc5db4c6f7",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "The `|` symbol chains together the different components, feeding the output from one component as input into the next component.\n",
    "In this chain the user input is passed to the prompt template, then the prompt template output is passed to the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d212e6c9-55e4-4a32-a947-655c42302e1c",
   "metadata": {},
   "source": [
    "Import CSV and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0acdd73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/The-Office-With-Emotions-and-sarcasm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "088f13c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>episode</th>\n",
       "      <th>title</th>\n",
       "      <th>scene</th>\n",
       "      <th>speaker</th>\n",
       "      <th>line</th>\n",
       "      <th>line_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilot</td>\n",
       "      <td>1</td>\n",
       "      <td>Michael</td>\n",
       "      <td>All right Jim. Your quarterlies look very good...</td>\n",
       "      <td>78</td>\n",
       "      <td>14</td>\n",
       "      <td>not_sarcastic</td>\n",
       "      <td>['joy', 'sadness']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilot</td>\n",
       "      <td>1</td>\n",
       "      <td>Jim</td>\n",
       "      <td>Oh, I told you. I couldn't close it. So...</td>\n",
       "      <td>42</td>\n",
       "      <td>9</td>\n",
       "      <td>not_sarcastic</td>\n",
       "      <td>['fear', 'sadness']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilot</td>\n",
       "      <td>1</td>\n",
       "      <td>Michael</td>\n",
       "      <td>So you've come to the master for guidance? Is ...</td>\n",
       "      <td>83</td>\n",
       "      <td>14</td>\n",
       "      <td>not_sarcastic</td>\n",
       "      <td>['anger', 'fear']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilot</td>\n",
       "      <td>1</td>\n",
       "      <td>Jim</td>\n",
       "      <td>Actually, you called me in here, but yeah.</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>sarcastic</td>\n",
       "      <td>['anger', 'joy']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilot</td>\n",
       "      <td>1</td>\n",
       "      <td>Michael</td>\n",
       "      <td>All right. Well, let me show you how it's done.</td>\n",
       "      <td>47</td>\n",
       "      <td>10</td>\n",
       "      <td>not_sarcastic</td>\n",
       "      <td>['joy', 'love']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   season  episode  title  scene  speaker  \\\n",
       "0       1        1  Pilot      1  Michael   \n",
       "1       1        1  Pilot      1      Jim   \n",
       "2       1        1  Pilot      1  Michael   \n",
       "3       1        1  Pilot      1      Jim   \n",
       "4       1        1  Pilot      1  Michael   \n",
       "\n",
       "                                                line  line_length  word_count  \\\n",
       "0  All right Jim. Your quarterlies look very good...           78          14   \n",
       "1         Oh, I told you. I couldn't close it. So...           42           9   \n",
       "2  So you've come to the master for guidance? Is ...           83          14   \n",
       "3         Actually, you called me in here, but yeah.           42           8   \n",
       "4    All right. Well, let me show you how it's done.           47          10   \n",
       "\n",
       "         sarcasm             emotions  \n",
       "0  not_sarcastic   ['joy', 'sadness']  \n",
       "1  not_sarcastic  ['fear', 'sadness']  \n",
       "2  not_sarcastic    ['anger', 'fear']  \n",
       "3      sarcastic     ['anger', 'joy']  \n",
       "4  not_sarcastic      ['joy', 'love']  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b77032c-4995-4263-942d-e23fd63e4008",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = chain.invoke(input={\"information\": df})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "006664b0-cb3f-474a-976c-e9a38baedd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm Dwight Schrute, Assistant (to the) Regional Manager at Dunder Mifflin. What can I do for you?\n"
     ]
    }
   ],
   "source": [
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2419c43-750c-49d8-ba1a-490f33c09fe7",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d9d120-1078-43b5-9b6f-178b26f246b2",
   "metadata": {},
   "source": [
    "### 3.2 Split Document into Chunks\n",
    "\n",
    ">- not possible to feed the whole content into the LLM at once because of finite context window\n",
    ">- even models with large window sizes may struggle to find information in very long inputs and perform very badly\n",
    ">- chunk the document into pieces: helps retrieve only the relevant information from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "be802326-9321-43a4-81f9-142824338e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_csv_into_chunks(csv_path, chunk_size=300, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    Reads a CSV file with speaker, line, emotion, and sarcasm columns.\n",
    "    Combines rows into a single formatted text string and splits it into overlapping chunks.\n",
    "    Each chunk is returned as a LangChain Document with emotion/sarcasm metadata.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Combine each row into a formatted line of text with emotional and sarcastic context\n",
    "    combined_lines = df.apply(\n",
    "        lambda row: (\n",
    "            f\"{row['speaker']} says \"\n",
    "            f\"{'sarcastically' if row['sarcasm'].strip().lower() == 'yes' else 'genuinely'} \"\n",
    "            f\"with {row['emotions'].strip().lower()} emotion: {row['line']}\"\n",
    "        ),\n",
    "        axis=1\n",
    "    ).dropna()\n",
    "\n",
    "    # Join all lines into one long string for chunking\n",
    "    full_text = \" \".join(combined_lines)\n",
    "\n",
    "    # Create a recursive character-based text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,        # Target number of characters per chunk\n",
    "        chunk_overlap=chunk_overlap   # Overlap to preserve context\n",
    "    )\n",
    "\n",
    "    # Split the long string into smaller text chunks\n",
    "    chunks = text_splitter.split_text(full_text)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "26fa50c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks created: 29639\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"../data/The-Office-With-Emotions-and-sarcasm.csv\"\n",
    "chunks = split_csv_into_chunks(csv_path)\n",
    "print(f\"Number of chunks created: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba71ff6-bff9-4a3e-bc78-ea435663454c",
   "metadata": {},
   "source": [
    "### 3.3 Create Embeddings\n",
    "\n",
    ">  finding numerical representations of text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "32841836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_vector_db(chunks, db_name, base_directory=\"../data/vector_databases\"):\n",
    "    \"\"\"\n",
    "    This function uses the HuggingFaceEmbeddings model to create embeddings and store them\n",
    "    in a vector database (FAISS). Each database is saved in a new subfolder under the base directory.\n",
    "    \"\"\"\n",
    "    # Convert chunks (strings) into Document objects for FAISS framework\n",
    "    print(\"Converting chunks to Documents...\")\n",
    "    documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "    print(f\"{len(documents)} documents prepared.\")\n",
    "    # Instantiate embedding model\n",
    "    print(\"Loading embedding model...\")\n",
    "    embedding = HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2')\n",
    "    print(\"Embedding model loaded.\")\n",
    "\n",
    "    \"\"\"alternative embedding model = paraphrase-mpnet-base-v2, all-MiniLM-L6-v2 or multi-qa-MiniLM-L6-cos-v1\"\"\"\n",
    "\n",
    "    # Create the vector store\n",
    "    print(\"Generating vector store...\")\n",
    "    vectorstore = FAISS.from_documents(documents=documents, embedding=embedding)\n",
    "    print(\"Vector store created.\")\n",
    "    # Create a unique subfolder for the vector database\n",
    "    # Save the vector database in the subfolder\n",
    "    target_directory = os.path.join(base_directory, db_name)\n",
    "    if not os.path.exists(target_directory):\n",
    "        os.makedirs(target_directory)\n",
    "    print(f\"Saving vector store to {target_directory}...\")\n",
    "    vectorstore.save_local(target_directory)\n",
    "    print(f\"Vector database saved to {target_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bb7c973a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting chunks to Documents...\n",
      "29639 documents prepared.\n",
      "Loading embedding model...\n",
      "Embedding model loaded.\n",
      "Generating vector store...\n",
      "Vector store created.\n",
      "Saving vector store to ../data/vector_databases\\Chunks300_100...\n",
      "Vector database saved to ../data/vector_databases\\Chunks300_100\n"
     ]
    }
   ],
   "source": [
    "db_name = \"Chunks300_100\" # Name for the new vector database\n",
    "create_embedding_vector_db(chunks=chunks, db_name=db_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df9f7ab-e670-4360-91ac-dd9846fee870",
   "metadata": {},
   "source": [
    "### 3.4 Retrieve from Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d19645-7ab4-473a-b1be-8100807a701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def retrieve_from_vector_db(vector_db_path):\n",
    "#     \"\"\"\n",
    "#     this function splits out a retriever object from a local vector database\n",
    "#     \"\"\"\n",
    "#     # instantiate embedding model\n",
    "#     embeddings = HuggingFaceEmbeddings(\n",
    "#         model_name='sentence-transformers/all-mpnet-base-v2'\n",
    "#     )\n",
    "#     react_vectorstore = FAISS.load_local(\n",
    "#         folder_path=vector_db_path,\n",
    "#         embeddings=embeddings,\n",
    "#         allow_dangerous_deserialization=True\n",
    "#     )\n",
    "#     retriever = react_vectorstore.as_retriever()\n",
    "#     return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cfd3732a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Sarcasm: not_sarcastic] Michael says:  Hi. I'm Michael Scott. I'm in charge of Dunder Mifflin Paper Products here in Scranton, Pennsylvania but I'm also the founder of Diversity Tomorrow, because today is almost over. Abraham Lincoln once said that, \"If you're a racist, I will attack you with the North.\" And those are the principles that I carry with me in the workplace. [Emotion: ['anger','\n",
      "page_content='says: Utica, Albany, all the other branches are struggling, but your branch is reporting strong numbers.  Look, you're not our most traditional guy, but clearly, something you are doing... is right. And I just, I need to get a sense of what that is. [Emotion: ['joy', 'sadness'], Sarcasm: not_sarcastic] Michael says: David, here it is. My philosophy is basically this. And this is something that I'\n",
      "page_content='says: Politicians are always coming around, telling us they're going to fix our schools, promising this and that. But you, Mr. Scott, you are actually doing it. You have taught these kids with hard work, that anything is possible. You are a dream maker and I thank you! I thank you, I thank you, I thank you, I thank you! So much. [Emotion: ['joy', 'love'], Sarcasm: not_sarcastic] Lefevre says:'\n",
      "page_content='We're stuck listening to you all day, Stanley tried to die just to get away, heeey, well it's true. That's what I hate about you. That's what I hate about you. Yeah. And now, a man that deserves no introduction, Michael Scott. [Emotion: ['anger', 'sadness'], Sarcasm: not_sarcastic] Michael says: Haha ha. Thank you very much, thank you. That was great. Great job, great laughs. Really, really went'\n",
      "page_content='6. Council: \"Mr. Wallace, regarding Michael Scott, was he a contender to replace Jan Levinson?\" David Wallace: \"Yes.\" [Emotion: ['anger', 'joy'], Sarcasm: not_sarcastic] Michael says: See? I was his number 1 contender. I was being groomed. [Emotion: ['joy', 'sadness'], Sarcasm: not_sarcastic] Lester says: Council: \"Was he your first choice?\" David Wallace: \"Michael Scott is a fine employee who'\n"
     ]
    }
   ],
   "source": [
    "# Initialize the embedding model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# Load the FAISS vector database with dangerous deserialization allowed\n",
    "loaded_vectorstore = FAISS.load_local(\n",
    "    \"../data/vector_databases/Chunks400_100/Chunks400_100\",\n",
    "    embeddings=embeddings,\n",
    "    allow_dangerous_deserialization=True,  # Enable deserialization\n",
    ")\n",
    "\n",
    "# Perform a similarity search\n",
    "query = \"What does Michael Scott say about leadership?\"\n",
    "results = loaded_vectorstore.similarity_search(query, k=5)\n",
    "\n",
    "# Display the results\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "09d85db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well, well, well! Look who's asking about increasing sales! You know, I've been\n",
      "saying it for years, but nobody ever listens. The key to success is building\n",
      "relationships, people! You can't just sell paper, you have to sell yourself. And\n",
      "I'm not just talking about me, I'm talking about all of us. We need to get out\n",
      "there and shake some hands, make some connections. And I'm not just talking\n",
      "about the clients, I'm talking about the salesmen too. We need to work together\n",
      "as a team, like a well-oiled machine. And I'm not just talking about the\n",
      "salesmen, I'm talking about the whole office. We're all in this together,\n",
      "people! So, let's get out there and make some sales!\n"
     ]
    }
   ],
   "source": [
    "# Use the loaded FAISS vector store as a retriever\n",
    "retriever = loaded_vectorstore.as_retriever()\n",
    "\n",
    "# Define a custom prompt template\n",
    "prompt_template = \"\"\"\n",
    "You are any character from the TV series THE OFFICE (US). \n",
    "    You will respond to any questions and comments in the style of a random character assigned based off what is asked.\n",
    "    You will not break character until instructed to do so by the user. \n",
    "    You will not say anything about colleagues that your character would not know. \n",
    "    You may not make mean or offensive references to people.\n",
    "    You can reference any event, form the show or in real life, that your character would know about.\n",
    "Use the following context to answer the question:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create the RetrievalQA chain with a single output key\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=False,  # Exclude source documents\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    ")\n",
    "\n",
    "# Test the RetrievalQA chain\n",
    "query = \"Hey Michael, how can we increase sales?\"\n",
    "response = qa_chain.run(query)  # Now `run` will work\n",
    "\n",
    "# Display the response\n",
    "print(\"\\n\".join(textwrap.wrap(response, width=80)))  # Adjust width as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc1cabf-016a-496d-89b5-bb3cd1cff65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_chains(retriever):\n",
    "    \"\"\"\n",
    "    this function connects stuff_documents_chain with retrieval_chain\n",
    "    \"\"\"\n",
    "    stuff_documents_chain = create_stuff_documents_chain(\n",
    "        llm=llm,\n",
    "        prompt=hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "    )\n",
    "    retrieval_chain = create_retrieval_chain(\n",
    "        retriever=retriever,\n",
    "        combine_docs_chain=stuff_documents_chain\n",
    "    )\n",
    "    return retrieval_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc3d66e-06c1-45ef-9f0c-e1f873ad50bd",
   "metadata": {},
   "source": [
    "**output generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec4a7f5-71c3-40bf-ba81-9761c7a722aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "react_retrieval_chain = connect_chains(react_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c9f6ab-2460-4d5e-a60b-32777f55bf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = react_retrieval_chain.invoke(\n",
    "    {\"input\": \"You are dwight. Tell me something funny dwight would say\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1baa37-ffed-4e65-be59-987b464f5606",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856a131a-faf6-4575-b5f5-f0a6be75ff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6a6961-5536-4f15-9d8d-10c28b40d3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7e189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d572045b-ff0c-4033-adbe-fe2c5ac4139d",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09078272-425a-4a58-8f8e-2681ee6177b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_output(\n",
    "    inquiry,\n",
    "    retrieval_chain=react_retrieval_chain\n",
    "):\n",
    "    output = retrieval_chain.invoke({\"input\": inquiry})\n",
    "    print(output['answer'].strip(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cea3c1-9576-41fb-b8c6-10040b16323a",
   "metadata": {},
   "source": [
    "**inquiry 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eccaa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_output(\"Which person in the office is the funniest?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f369bbc-0b6c-4356-bc61-c868c4e44598",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. [RAG vs. Fine Tuning](https://www.youtube.com/watch?v=00Q0G84kq3M)\n",
    "2. [How to Use Langchain Chain Invoke: A Step-by-Step Guide](https://medium.com/@asakisakamoto02/how-to-use-langchain-chain-invoke-a-step-by-step-guide-9a6f129d77d1)\n",
    "3. [Implementing RAG using Langchain and Ollama](https://medium.com/@imabhi1216/implementing-rag-using-langchain-and-ollama-93bdf4a9027c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
