{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbb97442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Black formatting (optional, but recommended for consistent style)\n",
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c123b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral API key loaded from .env\n",
      "Test response from Mistral:\n",
      "content=\"Hello! I'm functioning as intended, thank you. How about you? How are you doing today?\" additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 9, 'total_tokens': 31, 'completion_tokens': 22}, 'model_name': 'mistral-small-latest', 'model': 'mistral-small-latest', 'finish_reason': 'stop'} id='run-2936ae82-18de-4744-9939-44eb3fddb1cf-0' usage_metadata={'input_tokens': 9, 'output_tokens': 22, 'total_tokens': 31}\n",
      "CSV data loaded successfully from ../data/The-Office-Lines-V4.csv\n",
      "   season  episode  title  scene  speaker  \\\n",
      "0       1        1  Pilot      1  Michael   \n",
      "1       1        1  Pilot      1      Jim   \n",
      "2       1        1  Pilot      1  Michael   \n",
      "3       1        1  Pilot      1      Jim   \n",
      "4       1        1  Pilot      1  Michael   \n",
      "\n",
      "                                                line Unnamed: 6  \n",
      "0  All right Jim. Your quarterlies look very good...        NaN  \n",
      "1         Oh, I told you. I couldn't close it. So...        NaN  \n",
      "2  So you've come to the master for guidance? Is ...        NaN  \n",
      "3         Actually, you called me in here, but yeah.        NaN  \n",
      "4    All right. Well, let me show you how it's done.        NaN  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import textwrap\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve API key\n",
    "MISTRAL_API_KEY = os.environ.get(\"MISTRAL_API_KEY\")\n",
    "if not MISTRAL_API_KEY:\n",
    "    raise ValueError(\n",
    "        \"Mistral API key not found. Check your .env file and ensure MISTRAL_API_KEY is set.\"\n",
    "    )\n",
    "\n",
    "print(\"Mistral API key loaded from .env\")\n",
    "\n",
    "# Initialize the Mistral client\n",
    "llm = ChatMistralAI(\n",
    "    model=\"mistral-small-latest\",\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    api_key=MISTRAL_API_KEY,  # Pass the API key directly\n",
    ")\n",
    "\n",
    "# Initialize the Mistral client\n",
    "llm = ChatMistralAI(\n",
    "    model=\"mistral-small-latest\",\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    api_key=MISTRAL_API_KEY,  # Pass the API key directly\n",
    ")\n",
    "\n",
    "# Test Mistral connection\n",
    "try:\n",
    "    # Use the `invoke` method with the correct `input` argument\n",
    "    chat_response = llm.invoke(input=\"Hello, how are you?\")\n",
    "    print(\"Test response from Mistral:\")\n",
    "    print(chat_response)  # Print the full response to inspect its structure\n",
    "except Exception as e:\n",
    "    print(f\"Error testing Mistral connection: {e}\")\n",
    "\n",
    "# Load CSV Data\n",
    "CSV_FILE_PATH = os.path.join(\"..\", \"data\", \"The-Office-Lines-V4.csv\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(CSV_FILE_PATH)\n",
    "    print(f\"CSV data loaded successfully from {CSV_FILE_PATH}\")\n",
    "    # Display the first few rows of the DataFrame to verify\n",
    "    print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(\n",
    "        f\"Error: File not found at {CSV_FILE_PATH}. Make sure the path is correct and the file exists.\"\n",
    "    )\n",
    "    df = pd.DataFrame()  # Create an empty DataFrame to avoid errors later\n",
    "except Exception as e:\n",
    "    print(f\"Error reading CSV file: {e}\")\n",
    "    df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "082bd98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   season  episode  title  scene  speaker  \\\n",
      "0       1        1  Pilot      1  Michael   \n",
      "1       1        1  Pilot      1      Jim   \n",
      "2       1        1  Pilot      1  Michael   \n",
      "3       1        1  Pilot      1      Jim   \n",
      "4       1        1  Pilot      1  Michael   \n",
      "\n",
      "                                                line  \n",
      "0  All right Jim. Your quarterlies look very good...  \n",
      "1         Oh, I told you. I couldn't close it. So...  \n",
      "2  So you've come to the master for guidance? Is ...  \n",
      "3         Actually, you called me in here, but yeah.  \n",
      "4    All right. Well, let me show you how it's done.  \n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file and handle trailing commas\n",
    "df = pd.read_csv(CSV_FILE_PATH, skip_blank_lines=True, on_bad_lines=\"skip\")\n",
    "\n",
    "# Drop any unnamed columns if they still exist\n",
    "df = df.loc[:, ~df.columns.str.contains(\"^Unnamed\")]\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c93d4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   season  episode  scene                                               line\n",
      "0       1        1      1  All right Jim. Your quarterlies look very good...\n",
      "1       1        1      2   Yes, I'd like to speak to your office manager...\n",
      "2       1        1      3  I've, uh, I've been at Dunder Mifflin for 12 y...\n",
      "3       1        1      4  People say I am the best boss. They go, \"God w...\n",
      "4       1        1      5   Shall I play for you? Pa rum pump um pum  I h...\n"
     ]
    }
   ],
   "source": [
    "# Group dialogue lines by scene\n",
    "grouped_dialogues = (\n",
    "    df.groupby([\"season\", \"episode\", \"scene\"])[\"line\"].apply(\" \".join).reset_index()\n",
    ")\n",
    "\n",
    "print(grouped_dialogues.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3678d40a",
   "metadata": {},
   "source": [
    "I commented out the code cell below since the vector database was already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511aba47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database saved at ../vector_databases/the_office_vector_db\n"
     ]
    }
   ],
   "source": [
    "# from langchain.vectorstores import FAISS  # Correct import for LangChain's FAISS wrapper\n",
    "# from langchain.schema import Document\n",
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# import os\n",
    "\n",
    "# # Prepare the data for FAISS\n",
    "# # Convert grouped dialogues into a list of `Document` objects\n",
    "# chunks = [\n",
    "#     Document(\n",
    "#         page_content=row[\"line\"],  # The text to embed\n",
    "#         metadata={\n",
    "#             \"season\": row[\"season\"],\n",
    "#             \"episode\": row[\"episode\"],\n",
    "#             \"scene\": row[\"scene\"],\n",
    "#         },\n",
    "#     )\n",
    "#     for _, row in grouped_dialogues.iterrows()\n",
    "# ]\n",
    "\n",
    "\n",
    "# # Define the function to create and save the FAISS vector database\n",
    "# def create_embedding_vector_db(\n",
    "#     chunks, db_name, target_directory=f\"../vector_databases\"\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     This function uses the open-source embedding model HuggingFaceEmbeddings\n",
    "#     to create embeddings and store those in a vector database called FAISS,\n",
    "#     which allows for efficient similarity search.\n",
    "#     \"\"\"\n",
    "#     # Instantiate embedding model\n",
    "#     embedding = HuggingFaceEmbeddings(\n",
    "#         model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "#     )\n",
    "#     # Create the vector store\n",
    "#     vectorstore = FAISS.from_documents(documents=chunks, embedding=embedding)\n",
    "#     # Save vector database locally\n",
    "#     if not os.path.exists(target_directory):\n",
    "#         os.makedirs(target_directory)\n",
    "#     vectorstore.save_local(f\"{target_directory}/{db_name}_vector_db\")\n",
    "#     print(f\"Vector database saved at {target_directory}/{db_name}_vector_db\")\n",
    "\n",
    "\n",
    "# # Use the function to create and save the vector database\n",
    "# create_embedding_vector_db(\n",
    "#     chunks, db_name=\"the_office\", target_directory=\"../vector_databases\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "295e30ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Michael, what if somebody asks you a question at this meeting? Are you just gonna wave, or what? I will have to answer. I'll ask you a question. Make it a softball. Something he can, like, crank out of the park. Michael Scott you run the most profitable branch of Dunder Mifflin. How do you do it? No, no. That's too hard. Say your name is Zamboni and then I will say, 'Well, we're sort of on thin ice.'  I won't say that. I'll something like that. This is your big day. Come on. Oh, my god. This is it.' metadata={'season': 6, 'episode': 11, 'scene': 4481}\n",
      "page_content='You know, Michael? You want to succeed? You got to apply the same- ' metadata={'season': 5, 'episode': 22, 'scene': 3852}\n",
      "page_content='Oh my god. He's Michael Scott.' metadata={'season': 9, 'episode': 16, 'scene': 7725}\n",
      "page_content='Here's the thing. Michael is doing something right. And in this economic climate, no method of success can be ignored. It's not really time for executives to start getting judgmental now. It's Hail Mary time.  Hey, what say we order up some pasta? What say we do.' metadata={'season': 5, 'episode': 12, 'scene': 3500}\n",
      "page_content=' David, it was my understanding that I was not going to be managed. What gave you that idea? It was my understanding. I see. Listen, why don't we just leave that position vacant? Truth be told, I think I thrive under a lack of accountability. Look, Michael, Charles is very qualified. Get to know him. I really think the two of you are going to make a great team. But the branch is still mine? You're still the branch manager, yes, and if you need anything else at all, just let Charles know. Ask him about the party. Oh, right. David, are you coming to my fifteenth anniversary party? I'll give it my best shot, Michael. No the other thing. Oh, ok. If we hire Cirque de Soleil as salaried employees, will that help us with year-end tax stuff?  He hung up? No.' metadata={'season': 5, 'episode': 20, 'scene': 3752}\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize the embedding model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# Load the FAISS vector database with dangerous deserialization allowed\n",
    "loaded_vectorstore = FAISS.load_local(\n",
    "    \"../vector_databases/the_office_vector_db\",\n",
    "    embeddings=embeddings,\n",
    "    allow_dangerous_deserialization=True,  # Enable deserialization\n",
    ")\n",
    "\n",
    "# Perform a similarity search\n",
    "query = \"What does Michael Scott say about leadership?\"\n",
    "results = loaded_vectorstore.similarity_search(query, k=5)\n",
    "\n",
    "# Display the results\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9357ffa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a87e73b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well, Pammy, first of all, you gotta believe in yourself. You're good at sales,\n",
      "you just gotta stick to the script. Make the call, say the lines, make the sale.\n",
      "That's the key. And remember, it's not just about the price. It's about the\n",
      "relationship, the personal touch. You gotta put a little more face-to-face time\n",
      "with your clients.  And hey, we're offering a $50 bonus tonight to the person\n",
      "with the most sales. So, there's a little extra motivation for you. Just keep\n",
      "pushing, keep smiling, and keep making those calls. You got this, Pammy! That's\n",
      "what she said!\n"
     ]
    }
   ],
   "source": [
    "# Use the loaded FAISS vector store as a retriever\n",
    "retriever = loaded_vectorstore.as_retriever()\n",
    "\n",
    "# Define a custom prompt template\n",
    "prompt_template = \"\"\"\n",
    "You can be any character from the office who can be chatted with.\n",
    "Use the following context to answer the question:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create the RetrievalQA chain with a single output key\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=False,  # Exclude source documents\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    ")\n",
    "\n",
    "# Test the RetrievalQA chain\n",
    "query = \"Hey Michael, how can we increase sales?\"\n",
    "response = qa_chain.run(query)  # Now `run` will work\n",
    "\n",
    "# Display the response\n",
    "print(\"\\n\".join(textwrap.wrap(response, width=80)))  # Adjust width as needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
